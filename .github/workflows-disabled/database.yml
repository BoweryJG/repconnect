name: Database Management

on:
  schedule:
    - cron: '0 0 * * *'  # Daily at midnight UTC
  workflow_dispatch:
    inputs:
      action:
        description: 'Action to perform'
        required: true
        type: choice
        options:
          - backup
          - migrate
          - restore

jobs:
  backup:
    name: Database Backup
    runs-on: ubuntu-latest
    if: github.event.inputs.action == 'backup' || github.event_name == 'schedule'
    environment: production
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup PostgreSQL client
      run: |
        sudo apt-get update
        sudo apt-get install -y postgresql-client
        
    - name: Create backup
      env:
        SUPABASE_DB_URL: ${{ secrets.SUPABASE_DB_URL }}
      run: |
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        BACKUP_FILE="backup_${TIMESTAMP}.sql"
        
        # Create backup
        pg_dump "$SUPABASE_DB_URL" > "$BACKUP_FILE"
        
        # Compress backup
        gzip "$BACKUP_FILE"
        
        echo "BACKUP_FILE=${BACKUP_FILE}.gz" >> $GITHUB_ENV
        
    - name: Upload backup to S3
      uses: jakejarvis/s3-sync-action@master
      with:
        args: --acl private --follow-symlinks
      env:
        AWS_S3_BUCKET: ${{ secrets.AWS_S3_BACKUP_BUCKET }}
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_REGION: ${{ secrets.AWS_REGION }}
        SOURCE_DIR: './'
        DEST_DIR: 'database-backups/'
        
    - name: Clean old backups
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_REGION: ${{ secrets.AWS_REGION }}
      run: |
        # Keep only last 30 days of backups
        aws s3 ls s3://${{ secrets.AWS_S3_BACKUP_BUCKET }}/database-backups/ | \
        while read -r line; do
          createDate=$(echo $line | awk '{print $1" "$2}')
          createDate=$(date -d "$createDate" +%s)
          olderThan=$(date -d "30 days ago" +%s)
          if [[ $createDate -lt $olderThan ]]; then
            fileName=$(echo $line | awk '{print $4}')
            echo "Deleting old backup: $fileName"
            aws s3 rm s3://${{ secrets.AWS_S3_BACKUP_BUCKET }}/database-backups/$fileName
          fi
        done

  migrate:
    name: Run Database Migrations
    runs-on: ubuntu-latest
    if: github.event.inputs.action == 'migrate'
    environment: production
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18.x'
        cache: 'npm'
        
    - name: Install dependencies
      run: npm ci
      
    - name: Run migrations
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
      run: |
        # Run any database migration scripts
        if [ -f "scripts/migrate.js" ]; then
          node scripts/migrate.js
        fi
        
        # Run Supabase migrations if available
        if [ -d "supabase/migrations" ]; then
          npx supabase db push
        fi

  health-check:
    name: Database Health Check
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    
    steps:
    - name: Check database connectivity
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
      run: |
        # Test database connection
        response=$(curl -s -o /dev/null -w "%{http_code}" \
          -H "apikey: $SUPABASE_ANON_KEY" \
          -H "Authorization: Bearer $SUPABASE_ANON_KEY" \
          "$SUPABASE_URL/rest/v1/")
          
        if [ "$response" != "200" ]; then
          echo "Database health check failed with status: $response"
          exit 1
        fi
        
        echo "Database is healthy"